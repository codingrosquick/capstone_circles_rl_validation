{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports\n",
    "\n",
    "*notes:*\n",
    "- everything is sequentially made in this file because strymread can only be launched within jupyter notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.3\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "import strym\n",
    "from strym import strymread\n",
    "from strym import strymmap\n",
    "print(strym.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions\n",
    "\n",
    "## iRODS command wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ils():\n",
    "    '''\n",
    "    wrapper for iRODS ils command\n",
    "    :return: list of files and folder in the current folder\n",
    "    '''\n",
    "    process_files = subprocess.run(['ils'],\n",
    "                                   stdout=subprocess.PIPE,\n",
    "                                   stderr=subprocess.PIPE,\n",
    "                                   universal_newlines=True)\n",
    "    files = process_files.stdout.split(sep='\\n')[1:-1]\n",
    "    return [f.strip() for f in files]\n",
    "\n",
    "\n",
    "def icd(destination):\n",
    "    '''\n",
    "    wrapper for iRODS icd command\n",
    "    :param destination: destination to which go to\n",
    "    :return: subprocess output\n",
    "    '''\n",
    "    return subprocess.run(['icd', destination],\n",
    "                          stdout=subprocess.PIPE,\n",
    "                          stderr=subprocess.PIPE,\n",
    "                          universal_newlines=True)\n",
    "\n",
    "\n",
    "def ipwd():\n",
    "    '''\n",
    "    wrapper for iRODS ipwd command\n",
    "    :return: current directory on CyVerse\n",
    "    '''\n",
    "    pwd = subprocess.run(['ipwd'],\n",
    "                          stdout=subprocess.PIPE,\n",
    "                          stderr=subprocess.PIPE,\n",
    "                          universal_newlines=True)\n",
    "    out = pwd.stdout.strip().strip('\\n')\n",
    "    print('pwd output is:', out)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "async def async_command_shell(command, verbose: bool = False):\n",
    "    \"\"\"Run command in subprocess (shell).\n",
    "    source: https://fredrikaverpil.github.io/2017/06/20/async-and-await-with-subprocesses/\n",
    "    \"\"\"\n",
    "    # Create subprocess\n",
    "    process = await asyncio.create_subprocess_shell(command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)\n",
    "    # Status\n",
    "    if verbose:\n",
    "        print(\"Started:\", command, \"(pid = \" + str(process.pid) + \")\", flush=True)\n",
    "    # Wait for the subprocess to finish\n",
    "    stdout, stderr = await process.communicate()\n",
    "    # Output\n",
    "    if process.returncode == 0:\n",
    "        if verbose:\n",
    "            print(\"Done:\", command, \"(pid = \" + str(process.pid) + \")\", flush=True)\n",
    "        return stdout.decode().strip()\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"Failed:\", command, \"(pid = \" + str(process.pid) + \")\", flush=True)\n",
    "        raise Exception(stderr.decode().strip())\n",
    "\n",
    "\n",
    "async def iget(file_adress, destination, verbose: bool = False):\n",
    "    '''\n",
    "    wrapper for iRODS iget command\n",
    "    async command using asyncio library\n",
    "    :param file_adress: address on CyVerse fileshare\n",
    "    :param destination: address to download to on the local computer\n",
    "    :return: local address of the file\n",
    "    '''\n",
    "    try:\n",
    "        await async_command_shell(f'iget -T {file_adress} {destination}', verbose=verbose)\n",
    "        local_address = destination + '/' + file_adress.split('/')[-1]\n",
    "        return local_address\n",
    "    except Exception as e:\n",
    "        raise Exception(f'Error while downloading file at:'\n",
    "                        f'\\n\\tremote: {file_adress}'\n",
    "                        f'\\n\\tto local address: {destination}`'\n",
    "                        f'\\n\\tFailing on {e}')\n",
    "\n",
    "\n",
    "def init_cache(local_folder):\n",
    "    '''\n",
    "    clears the cache if exists and initialise it\n",
    "    :param local_folder: root folder for the analysis\n",
    "    :return: temporary cache address\n",
    "    '''\n",
    "    if local_folder != '':\n",
    "        subprocess.run(['cd', local_folder],\n",
    "                       stdout=subprocess.PIPE,\n",
    "                       stderr=subprocess.PIPE,\n",
    "                       universal_newlines=True)\n",
    "    local_folder_absolute = subprocess.run(['pwd'],\n",
    "                   stdout=subprocess.PIPE,\n",
    "                   stderr=subprocess.PIPE,\n",
    "                   universal_newlines=True).stdout.strip()\n",
    "    files = subprocess.run(['ls'],\n",
    "                   stdout=subprocess.PIPE,\n",
    "                   stderr=subprocess.PIPE,\n",
    "                   universal_newlines=True)\n",
    "    files = files.stdout.split(sep='\\n')\n",
    "    if 'temp_cache' in files:\n",
    "        subprocess.run(['rm', '-r', '-f', 'temp_cache'],\n",
    "                   stdout=subprocess.PIPE,\n",
    "                   stderr=subprocess.PIPE,\n",
    "                   universal_newlines=True)\n",
    "    subprocess.run(['mkdir', 'temp_cache'],\n",
    "               stdout=subprocess.PIPE,\n",
    "               stderr=subprocess.PIPE,\n",
    "               universal_newlines=True)\n",
    "    temp_cache_address = f'{local_folder_absolute}/temp_cache'\n",
    "    return temp_cache_address\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fileshare exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def findall_files(root, verbose: bool = False):\n",
    "    '''\n",
    "    finds all files within the root directory and recursively below\n",
    "    :param root: str, root file from which to begin the search\n",
    "    :param verbose: bool, set to True to see fuller logs\n",
    "    :return: List<str>\n",
    "    '''\n",
    "    dir_queue = [root]\n",
    "    files = []\n",
    "\n",
    "    while len(dir_queue) != 0:\n",
    "        current_dir = dir_queue.pop()\n",
    "        icd(current_dir)\n",
    "        queue = ils()\n",
    "        if verbose:\n",
    "            print('---------')\n",
    "            print('current queue dir: ', dir_queue)\n",
    "            print('current directory is: ', current_dir)\n",
    "            print('current file queue is: ', queue)\n",
    "\n",
    "        for f in queue:\n",
    "            if verbose:\n",
    "                print('current file tests on: ', f, ' and test gives f[0:2]: ', f[0:2], ' and f[-4:] is: ', f[-4:])\n",
    "            # avoid dashcams and bafiles folders, only use the libpanda ones -> reduces the number of files to scan for\n",
    "            if f[0:2] == 'C-' and 'bagfiles' not in f and 'dashcams' not in f:\n",
    "                dir_queue.append(f[3:])\n",
    "                if verbose:\n",
    "                    print('appending dir queue; ', f)\n",
    "            elif f[-4:] == '.csv':\n",
    "                # We also conserve the current folder to get the entire path to the file\n",
    "                current_folder = ipwd()\n",
    "                files.append(f'{current_folder}/{f}')\n",
    "                if verbose:\n",
    "                    print('appending file; ', f)\n",
    "\n",
    "        if verbose:\n",
    "            print('found ', len(files), ' files')\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def can_gps_coupling(files):\n",
    "    '''\n",
    "    links the CAN and GPS from same acquisitions\n",
    "    :param files: array of file adresses\n",
    "    :return: List<{'can': str, 'gps': str || None}>\n",
    "    '''\n",
    "    file_list = []\n",
    "    for file in files:\n",
    "        if '_CAN_Messages.csv' in file:\n",
    "            file_list.append({'can': file, 'gps': None})\n",
    "\n",
    "    for i in range(len(file_list)):\n",
    "        file_gps = file_list[i]['can'][0:-17] + '_GPS_Messages.csv'\n",
    "        if file_gps in files:\n",
    "            file_list[i]['gps'] = file_gps\n",
    "\n",
    "    return file_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Car crossing detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_metadata(canfile, gpsfile, ignore_gps_file: bool = False, verbose: bool = False):\n",
    "    \"\"\"\n",
    "    :param canfile: string, csv file\n",
    "    :param gpsfile: string, csv file\n",
    "    :param ignore_gps_file: set to True to avoid downloading the GPS file\n",
    "    :return: strymreads of can and gps files, dictionnary of the meta-datas associated\n",
    "    TODO: find how to extract the metadata here (maybe from strymread)?\n",
    "      Find also the desired metadata to be logged into the DB\n",
    "      -> what could be the useful thing to sort on? car model? day? other things?\n",
    "    \"\"\"\n",
    "\n",
    "    # read canfile\n",
    "    s = strymread(csvfile=canfile)\n",
    "    if verbose:\n",
    "        print(f'reading of {canfile} was succesful? {s.success}')\n",
    "\n",
    "    # read and link gpsfile\n",
    "    if not ignore_gps_file:\n",
    "        g = strymmap(csvfile=gpsfile)\n",
    "        if verbose:\n",
    "            print(f'reading of {gpsfile} was succesful? {g.success}')\n",
    "    else:\n",
    "        g = None\n",
    "        if verbose:\n",
    "            print(f'reading of GPS file was ignored')\n",
    "\n",
    "    # metadata from filename\n",
    "    date_time = canfile.split('/')[-1][0:19]\n",
    "    vin = canfile.split('/')[-1][20:37]\n",
    "\n",
    "    return s, g, {'date_time': date_time, 'vin': vin}\n",
    "\n",
    "\n",
    "def read_data(can, gps):\n",
    "    \"\"\"\n",
    "    :param can: strymread object\n",
    "    :param gps: strymmap object\n",
    "    :return: speed, lead_distance, cruise_control time series\n",
    "    \"\"\"\n",
    "    try:\n",
    "        speed_ts = can.speed()\n",
    "        lead_distance_ts = can.lead_distance()\n",
    "        cruise_control_state_ts = can.acc_state()\n",
    "        return speed_ts, lead_distance_ts, cruise_control_state_ts\n",
    "    except Exception as err:\n",
    "        print(f\"Error while trying to read the time series.\\nFailed on: {err}\")\n",
    "        raise Exception(err)\n",
    "\n",
    "\n",
    "def find_ts_state_at_given_time(ts, ts_time, event_time):\n",
    "    \"\"\"\n",
    "    Finds the value of a time series at a given point in time. Uses the closest time point to the event\n",
    "    :param ts: Time Series messages list to search within\n",
    "    :param ts_time: Time Series time list\n",
    "    :param event_time: Time at which we want the value\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    min_index = np.argmin([np.abs(time - event_time) for time in ts_time])\n",
    "    return ts[min_index]\n",
    "\n",
    "\n",
    "def find_crossing(speed, lead_distance, cruise_control_state, speed_treshold = 20,\n",
    "                  prev_treshold = 10, next_treshold = 5, verbose: bool = False):\n",
    "    \"\"\"\n",
    "    finds the time where car crossing events happens, from ts associated to 1 specific acquisition\n",
    "    this functions find the acceptable intervals where the constraints on speed and cruise control are valid,\n",
    "    then finds the places where crossings happens, filtering them by the acceptable times (this allows to handle\n",
    "    different sampling frequencies over the different time series)\n",
    "\n",
    "    :param speed: Time Series of the speed\n",
    "    :param lead_distance: Time Series of the Speed\n",
    "    :param cruise_control_state: Time Series of the Controller state (=6 if activated)\n",
    "    :param speed_treshold: minimum speed to consider a dangerous time crossing event, in km/h\n",
    "    :param prev_treshold: minimum lead distance before the crossing to consider the event as a car crossing\n",
    "    :param next_treshold: maximum lead distance after the crossing to consider the car crossing as dangerous\n",
    "    :param verbose: Set to true to get more logs\n",
    "\n",
    "    :return: array<time>, of event_times of car crossing events\n",
    "\n",
    "    TODO: refine the cruise control state to also encompass other semi-activated states as if controller on?\n",
    "    \"\"\"\n",
    "    event_times = []\n",
    "    controller_states = []\n",
    "    speeds = []\n",
    "    unacceptable_crossings = []\n",
    "\n",
    "    lead_distance_list = lead_distance['Message']\n",
    "    lead_time_list = lead_distance['Time']\n",
    "    len_lead = len(lead_time_list)\n",
    "\n",
    "    speed_list = speed['Message']\n",
    "    speed_time_list = speed['Time']\n",
    "    len_speed = len(speed_time_list)\n",
    "\n",
    "    cc_state_list = cruise_control_state['Message']\n",
    "    cc_state_time_list = cruise_control_state['Time']\n",
    "\n",
    "    # Acceptable times for cruise control state and speed:\n",
    "    # composed of time interval objects {\"beg\": time_begining, \"end\": time_ending}\n",
    "    acceptable_range_speed = []\n",
    "    currently_valid = False\n",
    "    current_interval = {\"beg\": None, \"end\": None}\n",
    "    for i in range(len_speed):\n",
    "        if speed_list[i] >= speed_treshold and not currently_valid:\n",
    "            currently_valid = True\n",
    "            current_interval['beg'] = speed_time_list[i]\n",
    "        elif speed_list[i] < speed_treshold and currently_valid:\n",
    "            currently_valid = False\n",
    "            current_interval['end'] = speed_time_list[i]\n",
    "            acceptable_range_speed.append(current_interval)\n",
    "            current_interval = {\"beg\": None, \"end\": None}\n",
    "\n",
    "    # case if the speed is still acceptable at the end of the file:\n",
    "    if current_interval['beg'] and not current_interval['end']:\n",
    "        current_interval['end'] = speed_time_list[-1]\n",
    "        acceptable_range_speed.append(current_interval)\n",
    "\n",
    "    for i in range(1, len_lead):\n",
    "        is_lead_distance_acceptable = (lead_distance_list[i - 1] >= prev_treshold) and (lead_distance_list[i] <= next_treshold)\n",
    "        if is_lead_distance_acceptable:\n",
    "            # if at this time a car crossing occurs, we check that the conditions to store this event are valid\n",
    "            time_event = lead_time_list[i]\n",
    "            unacceptable_crossings.append(time_event)\n",
    "            for interval in acceptable_range_speed:\n",
    "                if interval['beg'] <= time_event <= interval['end']:\n",
    "                    event_times.append(time_event)\n",
    "                    controller_states.append(find_ts_state_at_given_time(cc_state_list, cc_state_time_list, time_event))\n",
    "                    speeds.append(find_ts_state_at_given_time(speed_list, speed_time_list, time_event))\n",
    "\n",
    "    if verbose:\n",
    "        print(f'acceptable range for speed > {speed_treshold} m/s: {acceptable_range_speed}')\n",
    "        print(f'number of crossings detected: {len(unacceptable_crossings)}')\n",
    "        print(f'number of valid crossings detected: {len(event_times)}')\n",
    "        print(f'event times of valid crossings: {event_times}')\n",
    "\n",
    "    return event_times, controller_states, speeds\n",
    "\n",
    "\n",
    "def plot_events_over_lead(name, lead, times, event_times):\n",
    "    \"\"\"\n",
    "    TODO document this\n",
    "    TODO use plotly instead of pyplot\n",
    "    :param lead:\n",
    "    :param times:\n",
    "    :param event_times:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # create a fake list of event times to see them on the graph\n",
    "    def fake(time):\n",
    "        if time in event_times:\n",
    "            return 252\n",
    "        else:\n",
    "            return 0\n",
    "    event_times_fake = [fake(time) for time in times]\n",
    "    # plot the figure\n",
    "    fig, ax = plt.subplots()\n",
    "    l = ax.plot(times, lead, 'b.')\n",
    "    e = ax.plot(times, event_times_fake, 'r-')\n",
    "    plt.title(name)\n",
    "    plt.ion()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File handler & cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FileHandler:\n",
    "    \"\"\"\n",
    "    Class handling download and delete of files to be analyzed\n",
    "    \"\"\"\n",
    "    # attributes\n",
    "    all_files = None\n",
    "    coupled_files = None\n",
    "    local_root_folder = None\n",
    "    remote_addresses = None\n",
    "    can_local_address = None\n",
    "    gps_local_address = None\n",
    "    index = None\n",
    "    max_index = None\n",
    "\n",
    "    # methods\n",
    "    def __init__(self, local_root_folder, start_index: int = 0):\n",
    "        \"\"\"\n",
    "        :param local_root_folder: Local root for the download folder\n",
    "        \"\"\"\n",
    "        self.local_root_folder = local_root_folder\n",
    "        self.index = start_index\n",
    "        print('File Handler ready for file exploration')\n",
    "\n",
    "    def explore(self, analyze: bool = True, root: str = '', exploration_name = None,\n",
    "                 previous_exploration_path = None, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Initialises the path objects, then file handler attributes\n",
    "\n",
    "        :param analyze: True if you want to explore files from CyVerse,\n",
    "        False if you want to use a file giving the coupled files from a previous FileShare exploration\n",
    "        :param root: root of the search for exploring on CyVerse\n",
    "        :param exploration_name: name for the coupled file local copy\n",
    "        :param previous_exploration_path: local address towards the file giving the coupled files from a previous\n",
    "        FileShare exploration\n",
    "\n",
    "        TODO: include a call to an iinit irods function\n",
    "        \"\"\"\n",
    "        # case of file share exploration\n",
    "        if analyze:\n",
    "            try:\n",
    "                self.all_files = findall_files(root, verbose)\n",
    "                self.coupled_files = can_gps_coupling(self.all_files)\n",
    "                self.max_index = len(self.coupled_files)\n",
    "                # save the csv file\n",
    "                output_filename = coupled_files_file_namer(exploration_name, root)\n",
    "                df = pd.DataFrame(data={'Files': self.coupled_files})\n",
    "                df.to_csv(path_or_buf=f'results/{output_filename}')\n",
    "                if verbose:\n",
    "                    print('exploration logged as: ', output_filename)\n",
    "            except Exception as e:\n",
    "                print(f'CyVerse FileShare exploration failed on: {e}')\n",
    "\n",
    "        # case of using a file to get the coupled addresses\n",
    "        else:\n",
    "            try:\n",
    "                df = pd.read_csv(previous_exploration_path)\n",
    "                self.coupled_files = df['Files']\n",
    "                self.max_index = len(self.coupled_files)\n",
    "            except Exception as e:\n",
    "                print(f'retrieving from file at {previous_exploration_path} failed on: {e}')\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.max_index is None:\n",
    "            return f'FileShare exploration is not finished'\n",
    "        else:\n",
    "            return f'file handler with {self.max_index} couples, current index is: {self.index}'\n",
    "\n",
    "\n",
    "    async def next(self, ignore_gps_file: bool = False):\n",
    "        \"\"\"\n",
    "        clears cache & downloads the next couple of files\n",
    "        :param: ignore_gps_file: set to True to avoid downloading the GPS file\n",
    "        :return: - object with paths to the downloaded CAN and GPS file\n",
    "        {'can': str, 'gps': str, 'remote_addresses': {'can': str, 'gps': str}}\n",
    "                 - if the maximum index is reached, returns an exception as:\n",
    "        Exception('max_index')\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.index < self.max_index:\n",
    "                cache = init_cache(self.local_root_folder)\n",
    "                next_file = self.coupled_files[self.index]\n",
    "                if type(next_file) == type('string'):\n",
    "                    self.remote_addresses = ast.literal_eval(next_file)\n",
    "                else:\n",
    "                    self.remote_addresses = self.coupled_files[self.index]\n",
    "\n",
    "                self.can_local_address = await iget(self.remote_addresses['can'], cache)\n",
    "                if ignore_gps_file:\n",
    "                    self.gps_local_address = None\n",
    "                else:\n",
    "                    self.gps_local_address = await iget(self.remote_addresses['gps'], cache)\n",
    "\n",
    "                self.index += 1\n",
    "\n",
    "                return {\n",
    "                    'can': self.can_local_address,\n",
    "                    'gps': self.gps_local_address,\n",
    "                    'remote_addresses': self.remote_addresses\n",
    "                }\n",
    "            else:\n",
    "                raise Exception('max_index')\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Downloading next file failed on {e}')\n",
    "\n",
    "    def clear(self):\n",
    "        init_cache(self.local_root_folder)\n",
    "        print('Cache cleared')\n",
    "\n",
    "def coupled_files_file_namer(name, root):\n",
    "    return f'file_exploration&{name}&create_on={str(datetime.now()).replace(\" \", \"_\")}&root={root.replace(\"/\", \"_\")}.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-file analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def csv_file_namer(name, speed_t, prev_t, next_t):\n",
    "    return f'{name}&create_on={datetime.now()}&s={speed_t}&p={prev_t}&n={next_t}.csv'\n",
    "\n",
    "\n",
    "def create_results_folder():\n",
    "    \"\"\"Wrapper for shell command \"mkdir results\". No return from this function.\"\"\"\n",
    "    subprocess.run(['mkdir', 'results'],\n",
    "                   stdout=subprocess.PIPE,\n",
    "                   stderr=subprocess.PIPE,\n",
    "                   universal_newlines=True)\n",
    "\n",
    "\n",
    "def analyse_events(canfile, gpsfile, prev_treshold, next_treshold, speed_treshold,\n",
    "                   verbose: bool = False, plot: bool = False, plot_name: str = ''):\n",
    "    \"\"\"\n",
    "    From a CAN and a GPS file as well as analysis parameters, finds the useful information (all of the events tiles,\n",
    "    as well as metadata about the acquisition and at event times)\n",
    "    :param canfile: local path to the CSV of the CAN acquisition \n",
    "    :param gpsfile: local path to the GPS of the CAN acquisition\n",
    "    :param prev_treshold: previous speed treshold for car crossing\n",
    "    :param next_treshold: next speed threshold for car crossing\n",
    "    :param speed_treshold: minimum speed to consider a car crossing\n",
    "    :param verbose: set to True to have more extensive logs\n",
    "    :param plot: set to True to plot the lead distance as well as the event times\n",
    "    :param plot_name: name to give to the plot\n",
    "    :return: array of:\n",
    "        - the event times\n",
    "        - cc states and speeds at time of the events\n",
    "        - metadata about the acquisition\n",
    "    all of those arrays reference the same event for the same indice \n",
    "    \"\"\"\n",
    "    ignore_gps_file = gpsfile is None\n",
    "    s, g, metadata = read_metadata(canfile, gpsfile, ignore_gps_file=ignore_gps_file)\n",
    "    speed, lead_dist, cc_state = read_data(s, g)\n",
    "    event_times, event_cc_states, event_speeds = find_crossing(speed, lead_dist, cc_state,\n",
    "                                                           prev_treshold=prev_treshold,\n",
    "                                                           next_treshold=next_treshold,\n",
    "                                                           speed_treshold=speed_treshold,\n",
    "                                                           verbose=verbose)\n",
    "    if plot:\n",
    "        strymread.plt_ts(lead_dist)\n",
    "        plot_events_over_lead(plot_name, lead_dist['Message'], lead_dist['Time'], event_times)\n",
    "    return event_times, event_cc_states, event_speeds, metadata\n",
    "\n",
    "\n",
    "async def explore_and_analyse_bdd(config_path, local_folder: str = '', start_index: int = 0):\n",
    "    \"\"\"\n",
    "    Launches the creation of the exploration and final analysis CSV (fake SQL databases)\n",
    "\n",
    "    :param config_path: path to a JSON giving the analysis' configuration\n",
    "    the format is as follows:\n",
    "    {\n",
    "        verbose: <bool>, set to True to have extensive logs\n",
    "        exploration:\n",
    "                    {\n",
    "                        remote_root: <str||None>, root of the fileshare exploration on CyVerse. Set to None\n",
    "                                    if you want to use the CSV file of a previous analysis\n",
    "                        db_exploration_name: <str||None>, small description of this exploration (an automated\n",
    "                                            logging of useful information as the date and the parameters is\n",
    "                                            already implemented). Set to None if you want to use the CSV file of a\n",
    "                                            previous analysis\n",
    "                        use_previous_exploration: <str||None>, if None, a new exploration is made using the\n",
    "                                                  parameters below. If a value is given, put the absolute location\n",
    "                                                  of the path for using a previous file exploration\n",
    "                    }\n",
    "        analysis:\n",
    "                    {\n",
    "                        db_analysis_name: <str>, small description of the full analysis. (automated\n",
    "                                        information are added)\n",
    "                        car_crossing_parameters:\n",
    "                            {\n",
    "                                enable: <bool>, set to True to look for this kind of events\n",
    "                                speed_threshold: <float>, minimum speed, in m/s\n",
    "                                previous_distance_threshold: <float>, minimum distance before the crossing\n",
    "                                next_distance_threshold: <float>, maximum distance after the crossing\n",
    "                                                        note: the crossing is detected as a discontinuity in\n",
    "                                                        lead_distance time series\n",
    "                            }\n",
    "                    }\n",
    "    }\n",
    "    note: the JSON object is intended to hold different analysis parameters, for different situations than car crossings\n",
    "    :return: finishes the whole exploration\n",
    "    \"\"\"\n",
    "    # Configuration reading\n",
    "    verbose = False\n",
    "    try:\n",
    "        print('Opening configuration file')\n",
    "        f = open(config_path)\n",
    "        config = json.load(f)\n",
    "        print('Configuration opened successfully')\n",
    "        if config['verbose']:\n",
    "            verbose = True\n",
    "            print('verbose is set to True, extensive logs will be displayed')\n",
    "        else:\n",
    "            print('verbose is set to False, logs will be scarce')\n",
    "    except Exception as e:\n",
    "        raise Exception(f'There was an issue trying to open the configuration file. \\nIt failed on: {e}')\n",
    "\n",
    "    # Exploration of the file share\n",
    "    if verbose:\n",
    "        print('opening the file handler')\n",
    "    file_handler = FileHandler(local_root_folder=local_folder, start_index=start_index)\n",
    "    if 'use_previous_exploration' in config['exploration'].keys() and config['exploration']['use_previous_exploration']:\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Using the previous exploration, located at {config[\"exploration\"][\"use_previous_exploration\"]}')\n",
    "        file_handler.explore(analyze=False,\n",
    "                             previous_exploration_path=config['exploration']['use_previous_exploration'],\n",
    "                             verbose=verbose)\n",
    "        if verbose:\n",
    "            print('Previous exploration has been red successfully')\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('starting to explore...')\n",
    "        file_handler.explore(analyze=True,\n",
    "                             root=config['exploration']['remote_root'],\n",
    "                             exploration_name=config['exploration']['db_exploration_name'],\n",
    "                             verbose=verbose)\n",
    "        if verbose:\n",
    "            print(f'...exploration finished. Find it at ./results/{config[\"exploration\"][\"db_exploration_name\"]}')\n",
    "\n",
    "    # Analysis of the events\n",
    "    full_analysis_csv_filename = csv_file_namer(config['analysis']['db_analysis_name'],\n",
    "                                                config['analysis']['car_crossing_parameters']['speed_threshold'],\n",
    "                                                config['analysis']['car_crossing_parameters']['previous_distance_threshold'],\n",
    "                                                config['analysis']['car_crossing_parameters']['next_distance_threshold'])\n",
    "\n",
    "    output_data = {'remote_addresses': [], 'event_time': [], 'event_speeds': [], 'event_cc_state': [], 'event_type': [], 'date_time': [], 'vin': []}\n",
    "    if verbose:\n",
    "        print('Starting the analysis of the events in the explored files')\n",
    "    for index in range(file_handler.index, file_handler.max_index):\n",
    "        print('\\nINDEX is ', index, ' out of ', file_handler.max_index)\n",
    "        try:\n",
    "            # download files\n",
    "            current_files = await file_handler.next(ignore_gps_file=True)\n",
    "            if config['verbose']:\n",
    "                print('starting analysis of the file: ', current_files['can'])\n",
    "            try:\n",
    "                # event analysis\n",
    "                event_times, event_cc_states, event_speeds, metadata = analyse_events(current_files['can'], current_files['gps'],\n",
    "                                                       prev_treshold=config['analysis']['car_crossing_parameters']['previous_distance_threshold'],\n",
    "                                                       next_treshold=config['analysis']['car_crossing_parameters']['next_distance_threshold'],\n",
    "                                                       speed_treshold=config['analysis']['car_crossing_parameters']['speed_threshold'],\n",
    "                                                       verbose=config['verbose'])\n",
    "                # add the metadata to the events\n",
    "                event_type = 'car_crossing'\n",
    "                number_of_events = len(event_times)\n",
    "                if number_of_events > 0:\n",
    "                    output_data['event_time'].extend(event_times)\n",
    "                    output_data['event_speeds'].extend(event_speeds)\n",
    "                    output_data['event_cc_state'].extend(event_cc_states)\n",
    "                    output_data['remote_addresses'].extend([current_files['remote_addresses']] * number_of_events)\n",
    "                    output_data['event_type'].extend([event_type] * number_of_events)\n",
    "                    output_data['date_time'].extend([metadata['date_time']] * number_of_events)\n",
    "                    output_data['vin'].extend([metadata['vin']] * number_of_events)\n",
    "\n",
    "            except Exception as err:\n",
    "                print(f'there was an issue trying to scan {current_files[\"can\"]}. \\nFailed on: {err}')\n",
    "        except Exception as err:\n",
    "            print(f'there was an issue trying to download file at index {index}. \\nFailed on: {err}')\n",
    "\n",
    "    # CSV logging\n",
    "    if config['verbose']:\n",
    "        print(f'starting to write the data to the CSV')\n",
    "    try:\n",
    "        df = pd.DataFrame(data=output_data)\n",
    "\n",
    "        df.to_csv(path_or_buf=f'results/analysis&{full_analysis_csv_filename}')\n",
    "        file_handler.clear()\n",
    "        out_message = f'analysis finished, find it at: {full_analysis_csv_filename}'\n",
    "        if config['verbose']:\n",
    "            print(out_message)\n",
    "        return out_message\n",
    "    except Exception as e:\n",
    "        error_msg = f'Error trying to write the CSV file. \\nFailed on : {e}'\n",
    "        print(df)\n",
    "        print(error_msg)\n",
    "        return Exception('error_msg')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# full analysis, to do remotely on CyVerse\n",
    "config_path_full = './config/full_analysis_config_after_exploration.json'\n",
    "await explore_and_analyse_bdd(config_path_full, '', start_index = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to perform a REMOTE LAUNCH on CyVerse\n",
    "\n",
    "1. Go on Discovery environment, launch a Jupyter-strym application\n",
    "\n",
    "2. Grab the repo via git clone in the terminal\n",
    "\n",
    "3. Log in to Cyverse\n",
    "\n",
    "to perform iinit command, start by log out of the current irods user with:\n",
    "```\n",
    "     ichmod -M\n",
    "```\n",
    "\n",
    "Then perform iinit normally, with:\n",
    "```\n",
    "     Enter the host name (DNS) of the server to connect to: data.cyverse.org\n",
    "     Enter the port number: 1247\n",
    "     Enter your irods user name: <your_cyverse_user_name>\n",
    "     Enter your irods zone: iplant\n",
    "     Enter your current iRODS password: <your_cyverse_password>\n",
    "```\n",
    "\n",
    "4. Create the JSON configuration file\n",
    "5. Launch the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}